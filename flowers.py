import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# matplotlib inline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import numpy as np

sns.set_palette("Set2")
iris = pd.read_csv("Iris.csv")
iris.head()

# ----------
# THE DATA
# ----------
sns.pairplot(iris.drop(['Id'], axis=1), hue="Species")

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 10))
sns.violinplot(x="Species", y="SepalLengthCm", data=iris, ax=ax1)
sns.violinplot(x="Species", y="SepalWidthCm", data=iris, ax=ax2)
sns.violinplot(x="Species", y="PetalLengthCm", data=iris, ax=ax3)
sns.violinplot(x="Species", y="PetalWidthCm", data=iris, ax=ax4)

# plt.show()

# ---------------------
# K-NEAREST NEIGHBOUR
# ---------------------

# # 1. declare the classifier
# # n_neighbours is the number of closest neighbours we should consider when making a "vote"
# knn = KNeighborsClassifier(n_neighbors=3)
#
# # 2. prepare the input variable x, and target output y
# x, y = iris.drop(['Id', 'Species'], axis=1), iris['Species']
#
# # 3. split the dataset into two parts, one for training, one for testing the model later
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)
#
# # 4. fit the model using the training data
# knn.fit(x_train, y_train)
#
# # 5. make prediction with the input from test data
# prediction = knn.predict(x_test)

# print('With KNN (K=3) accuracy is: ', knn.score(x_test, y_test))    # accuracy

# ------------
# NAIVE BAYES
# ------------

# nb = GaussianNB()
# x, y = iris.drop(['Id', 'Species'], axis=1), iris['Species']
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)
# nb.fit(x_train, y_train)
# prediction = nb.predict(x_test)
#
# print('With NB accuracy is: ', nb.score(x_test, y_test))    # accuracy

# ------------
# DECISION TREE
# ------------

# dtree = DecisionTreeClassifier(max_depth=5, min_samples_split=5, min_samples_leaf=5)
# x, y = iris.drop(['Id', 'Species'], axis=1), iris['Species']
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)
# dtree.fit(x_train, y_train)
# prediction = dtree.predict(x_test)
# tree.export_graphviz(dtree, out_file="tree.dot")
#
# print('With Decision tree accuracy is: ', dtree.score(x_test, y_test))  # accuracy

# --------------
# RANDOM FOREST
# --------------

# rf = RandomForestClassifier()
# x, y = iris.drop(['Id', 'Species'], axis=1), iris['Species']
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)
# rf.fit(x_train, y_train)
# prediction = rf.predict(x_test)
#
# print('With Random Forest accuracy is: ', rf.score(x_test, y_test))     # accuracy

# --------------
# SUPPORT VECTOR MACHINE
# --------------

X, y = iris.iloc[:, 1:3], pd.factorize(iris['Species'])[0]

# Define the boundaries for the graphs we will draw later
x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1
y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1
h = (x_max / x_min)/100
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))


# Defining a function that declare a SVM classifier, with different parameters, and make predictions
def make_mesh(kerneltype, Cval, gammaval="auto"):
    svc = SVC(kernel=kerneltype, C=Cval, gamma=gammaval)
    svc.fit(X, y)
    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    return Z


# # kernel type
# Z = make_mesh("rbf", 1, gammaval="auto")
# plt.subplot(1, 1, 1)
# plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
#
# plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
# plt.xlabel('Sepal length')
# plt.ylabel('Sepal width')
# plt.xlim(xx.min(), xx.max())
# plt.title('SVC with linear kernel')
# plt.show()

# gamma value: higher the value, more influence a single training example has and the more it will try to exactly fit
# the data
# fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,6))
# fig.suptitle('Different Gamma values')

# Z = make_mesh("rbf", 1, gammaval=1)
# ax1.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# ax1.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm,s=20, edgecolors='k')
# ax1.set_title("Gamma = 1")
#
# Z = make_mesh("rbf", 1, gammaval=10)
# ax2.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm,s=20, edgecolors='k')
# ax2.set_title("Gamma = 10")
#
# Z = make_mesh("rbf", 1, gammaval=100)
# ax3.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# ax3.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm,s=20, edgecolors='k')
# ax3.set_title("Gamma = 100")
# plt.show()

# C value: Higher the C value, the more it will try to classify all data correctly.
# Lower the C value, smoother is decision boundary
# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
# fig.suptitle('Different C values')
#
# Z = make_mesh("rbf", 1, gammaval="auto")
# ax1.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# ax1.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
# ax1.set_title("C = 1")
#
# Z = make_mesh("rbf", 10, gammaval="auto")
# ax2.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
# ax2.set_title("C = 10")
#
# Z = make_mesh("rbf", 100, gammaval="auto")
# ax3.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# ax3.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
# ax3.set_title("C = 100")
# plt.show()
